{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://github.com/ta-data-par/Jan2020_EverydayCode/tree/master/Week_8/UnsupervisedProject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "In the past two lessons, you have learned what Unsupervised Machine Learning is, what problems are suitable for a solution based on Unsupervised Machine Learning, how to apply Unsupervised Machine Learning, and you have practiced implementing the basic phases of a solution using Scikit-learn. Now is time to put all that conceptual and procedural knowledge to work by doing a larger project. Choose a problem domain that motivates you, and build a complete solution implementing all the phases you learned about in previous chapters. We provide some ideas of interesting problem domains in a dedicated section in this lesson, but we want you to be creative and adventurous, and explore other options as well. This lesson does not present any new material: everything you will need to complete this project was discussed on previous lessons.\n",
    "External Interface Requirements\n",
    "\n",
    "    Input requirement: capacity to read a dataset stored on disk.\n",
    "    Output requirement: report on optimal number of clusters, centroid coordinates and quality metric.\n",
    "    Output requirement: identifiers of classes corresponding to new instances classified by the model.\n",
    "\n",
    "Functional Requirements\n",
    "\n",
    "    The software must learn a clusterization a the dataset.\n",
    "    The software must use the learned clusterization to classify new problem instances.\n",
    "    The software must evaluate the quality of a clusterization.\n",
    "    The software must be flexible to work with different preconfigured amount of clusters.\n",
    "    The software must compare results using different numbers of clusters and determine which number of clusters is best.\n",
    "\n",
    "Technical Requirements\n",
    "\n",
    "    Use Python as programming language.\n",
    "    Use Pandas for reading the dataset into a Pandas dataframe.\n",
    "    Use Scikit-learn for training and testing the Machine Learning model.\n",
    "\n",
    "Necessary Deliverables\n",
    "\n",
    "    Python application that performs ETL, training, and testing.\n",
    "    Report containing quality metrics, and explanation of the dataset, and the experimental procedure (range of the different number of clusters that were tested, how the range was traversed, etc.).\n",
    "    Optional(Build a supervised model by attaching your labels(clusters) as Target)\n",
    "\n",
    "Suggestions to Get Started\n",
    "\n",
    "    Find an interesting dataset! Look in the Useful Resources section for sources of ideas.\n",
    "    If you do not find a pre-existing dataset on the problem domain that you like, be creative: consider building the dataset yourself and donating the dataset to one of the public Machine Learning repositories.\n",
    "    Break down the project into smaller tasks, for instance: importing the dataset, training, etc.\n",
    "    Decide whether you will create a single Python application or several Python applications.\n",
    "\n",
    "Potential Project Ideas\n",
    "\n",
    "    Segment smartphone users according to phone usage and apps installed.\n",
    "    Segment healthy person under 50 years of age according to their risk or propensity of suffering from Alzheimer's disease after 70 years of age.\n",
    "    Classify differnent customers from an Ecomerce data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting libraries, additional ones will be added when needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display options\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data import and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Aciago\\ih\\Week7_project\\data\\terror\\globalterrorismdb_0718dist.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = pd.DataFrame((df.isnull().sum() * 100 / len(df)).sort_values(ascending=False))\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:, df.isnull().mean() < .15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['event_date']  = (pd.to_datetime(df1['iday'].astype(str) + '-' +\n",
    "                                  df1['imonth'].astype(str) + '-' +\n",
    "                                  df1['iyear'].astype(str), errors='coerce'))\n",
    "\n",
    "# errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’\n",
    "# coerce will return NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna(axis=0)\n",
    "# df1 = df1.drop(['iyear','iday','imonth'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.gname.value_counts()\n",
    "#txt remove, country and region, targtype1_txt, \n",
    "# reshape date col into month=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 2 cols regarding victims into single one\n",
    "df1['Victims']=df1['nkill']+df1['nwound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['nkill','nwound'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.drop(['eventid'],axis=1,inplace=True)\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming cols to a more logical and readable format\n",
    "\n",
    "df1.rename(columns={'extended':'Extended', 'country':'Country_id','country_txt':'Country_txt',\n",
    "                    'region':'Region_id', 'region_txt':'Region_txt', 'provstate':'Provstate', 'city':'City',\n",
    "                    'success':'Success', 'doubtterr':'PossibleNonTA','suicide':'Suicide', 'attacktype1':'AttackType_id',\n",
    "                    'attacktype1_txt':'AttackType_txt', 'targtype1':'TargetType_id', 'target1':'Target',\n",
    "                    'nkill':'Killed','nwound':'Wounded','summary':'Summary', 'Target_type':'TargetType_txt',\n",
    "                    'targsubtype1':'TargetSubType_id', 'targsubtype1_txt':'TargetSubType_txt',\n",
    "                    'natlty1':'TO_Nation_id','natlty1_txt':'TO_Nation_txt', 'weaptype1':'WeaponType_id',\n",
    "                    'weapsubtype1':'WeaponSubType_id','weapsubtype1_txt':'WeaponSubType_txt',\n",
    "                    'weaptype1':'WeaponType_id', 'targtype1_txt':'TargetType_txt', 'multiple':'MultipleActs',\n",
    "                   'gname':'TO_name', 'weaptype1_txt':'WeaponType_txt','property':'PropertyDmg'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('terror_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw data viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(22,8))\n",
    "\n",
    "sns.countplot('iyear', data=df1, palette='RdYlGn_r', orient='h') # orient arg ignored?\n",
    "\n",
    "plt.title('Reported Acts of Terror per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of reports')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,6))\n",
    "sns.countplot('WeaponType_txt', data=df1, palette='RdYlGn', order=df1['WeaponType_txt'].value_counts().index)\n",
    "\n",
    "plt.title('Weapons used in Terrorism')\n",
    "plt.xlabel('Weapon type')\n",
    "plt.ylabel('Number of attacks')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15,6))\n",
    "sns.countplot('Region_txt', data=df1, palette='RdYlGn', order=df1['Region_txt'].value_counts().index)\n",
    "\n",
    "plt.title('Amount of Terrorism Acts per Region')\n",
    "plt.xlabel('Region name')\n",
    "plt.ylabel('Number of attacks')\n",
    "plt.xticks(rotation=35)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figsize=(15,6)\n",
    "df1.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will drop text columns that already have numerical representation in the dataset, cols w\\o meaning and datetime\n",
    "\n",
    "df2 = df1.drop(['eventid', 'iyear', 'imonth', 'iday', 'Country_txt', 'Region_txt',\n",
    "                'Provstate', 'AttackType_txt', 'TargetType_txt', 'Target','TargetSubType_txt',\n",
    "               'WeaponType_txt', 'TO_Nation_txt', 'WeaponSubType_txt', 'dbsource', 'event_date', 'City'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.dbsource.value_counts()\n",
    "df2.TO_name.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df2.TO_name.value_counts()\n",
    "\n",
    "df2['Org_claim'] = np.where(df2['TO_name'].isin(s.index[s >= 1560]), df2['TO_name'], 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(data=df2, columns=['Org_claim'], drop_first=True)\n",
    "df2.drop('TO_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling method\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df2.values\n",
    "x=data[:,0:8]\n",
    "y=data[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX=scaler.fit_transform(x)\n",
    "rescaledX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize method\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1=StandardScaler().fit(x)\n",
    "rescaledX1=scaler1.transform(x)\n",
    "rescaledX1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering - KMeans (sklearn.cluster import KMeans)\n",
    "\n",
    "# silhoutte score\n",
    "# from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=40)\n",
    "df2_clusters=kmeans.fit(rescaledX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids=kmeans.cluster_centers_\n",
    "labels=kmeans.labels_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any X and Y columns, hue by centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df2.Victims, y=df2.Country_id, hue=centroids);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(x=df2.weaptype1, y=df2.country, hue=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(x=df2.weaptype1, y=df2.success, hue=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(centroids[:,0],centroids[:,1],marker='x',s=150,linewidths=5,zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to viz actual clusters ffs?\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples=1500\n",
    "X,y=df2(['Country_id', 'TargetType_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cluster.KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer=KElbowVisualizer(model, k=(1,5))\n",
    "visualizer.fit(X)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stscaler = StandardScaler().fit(df2)\n",
    "data = stscaler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figsize=(15,10)\n",
    "plt.scatter(df2.TargetType_id, df2.Country_id)\n",
    "plt.xlabel(\"Type of target\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.title(\"test viz\")\n",
    "plt.show()\n",
    "#plt.savefig(\"results/wholesale.png\", format = \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(stscaler))\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(algorithm='auto', metric='euclidean', eps=10, min_samples=50, n_jobs=-1).fit(data)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sklearn.metrics.pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsc = DBSCAN(eps = 3, min_samples = 10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbsc.labels_\n",
    "core_samples = np.zeros_like(labels, dtype = bool)\n",
    "core_samples[dbsc.core_sample_indices_] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0,1, len(unique_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (label, color) in zip(unique_labels, colors):\n",
    "    class_member_mask = (labels == label)\n",
    "    xy = data[class_member_mask & core_samples]\n",
    "    plt.plot(xy[:,0],xy[:,1], 'o', markerfacecolor = color, markersize = 10)\n",
    "    \n",
    "    xy2 = data[class_member_mask & ~core_samples]\n",
    "    plt.plot(xy2[:,0],xy2[:,1], 'o', markerfacecolor = color, markersize = 5)\n",
    "plt.title(\"DBSCAN on Wholsesale data\")\n",
    "plt.xlabel(\"Grocery (scaled)\")\n",
    "plt.ylabel(\"Milk (scaled)\")\n",
    "#plt.savefig(\"results/dbscan_wholesale.png\", format = \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "test_data = np.asarraydf2['']\n",
    "n_samples = 1500\n",
    "random_state = 42\n",
    "X, y = df2(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred = KMeans(n_clusters=10, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropicly Distributed Blobs\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"Unequal Variance\")\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3,\n",
    "                random_state=random_state).fit_predict(X_filtered)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
    "plt.title(\"Unevenly Sized Blobs\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
